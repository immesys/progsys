# Hekaton: SQL Server's Memory-Optimized OLTP Engine
## Diaconu, Freedman, Ismert, Larson, Mittal Stonecipher, Verma, Zwilling
## Microsoft

### Erik Krogen

Hekaton introduces a number of new ideas: a mixture of in-memory and on-disk tables that can be accessed in the same manner, versioning with parallel garbage collection, native code compilation, commit dependencies, and continuous checkpointing. All of these are very cool features that seem to solve their intended problems very well. Commit dependencies make a lot of sense - transactions abort a very small portion of the time, so continuing optimistically under the assumption that a transaction will succeed seems reasonable. The continuous checkpointing also seems very intelligent - the main problem (that I can see) of checkpointing is the necessity to do some expensive operation all at once, but their clever data/delta stream system solves this issue.

This system reminds me a lot of Postgres - versioned records, garbage/archive collection, etc., except that there are a number of features here that should allow it to be more successful, like garbage collection happening continuously and in parallel, as well as the fact that it's working in memory so things should be fast. So this begs the question - if you wanted a fully historical database like the original imagination of Postgres, could this system be easily and modified to achieve that while still being efficient? It seems that it lends itself to this. 

### Johann Schleier-Smith

This paper describes in some detail the considerations for a commercial implementation of an in-memory OLTP engine. There are lots of details, including those of integrating with a traditional SQL database, the approach to generating native code, and details of the timestamp-oriented multiversion concurrency control mechanisms. The authors share quite a lot of learnings, which though thoughtful are largely incremental. In some cases, say with respect to code generation, I was surprised to learn that such techniques are not already commonplace.

Whereas other recent in-memory database designs partition the data so that each region is accessed by one CPU only, in Hekaton all CPUs access the entire database. The authors claim that this reduces the need for application partitioning. This is certainly desirable from a user perspective, as administering partitioning can be burdensome and can distort schema from that which represents the problem most naturally.

There are two themes that contribute to the dramatic performance improvements over previous database storage engines: focus on cutting CPU instructions, and focus on eliminating latching or locking. What isn't clear from the paper is whether these are of comparable importance under ordinary operating circumstances. I am left wondering how much one improvement or the other would separately contribute, e.g., suppose they had retained locks or latches but compiled to native code to reduce CPU use. With faster code latches are held for shorter periods of time, so contention goes down as well. How much improvement latch-free data structures add is unclear.

Drawing conclusions from the experimental results is a bit challenging, as usual, because details of hardware and workload make a tremendous impact. The reported throughput, which goes as high as 36,000 transactions per second, is not particularly impressive. VoltDB has published benchmarks approaching 1 million transactions per second. My suspicion is that the system is bottlenecked outside of the transactional engine, perhaps in legacy code for initiating queries or returning results.