# Notes for 9-18-15: Hekaton and Bw-trees

## Hekaton paper presented by Ethan
- Joe: Main point of this was to get higher CPU utilization, previous databases already
  did a good job of catching the 'hot' records in RAM, but putting the whole thing in memory
  allowed them to do a much better job of utilizing the CPU (e.g. with compiling the stored procedures)
- Latch vs Lock: locks are slow, you have to look them up in a lock table, correspond to a specific
  object/record, much more heavyweight, hold them for entire transaction. Latches correspond to e.g.
  a page, hold them just as long as you need, live near the object you're latching
- Joe: Compilation process focuses on removing a lot of boilerplate type stuff, e.g. with traversal overheads
- Gabe: Are they recreating everything from the beginning of the checkpoint streams each time?
    - Everything is recovered from the data/delta streams to speed up recovery
    - Note that these are time partitioned 
    - Log is source of truth
- Checkpointing: data/delta streams
    - Checkpoint time is constantly moving, "continuous checkpointing"
    - Data file can be closed at some point, delta could potentially live forever
    - Multiple threads can be loading different data/delta file pairs simultaneously
    - Question: Why have separate delta and data files?
        - Nice because data file can be closed, delta file (much smaller) lives on
        - Maybe motivated by the way the SQL file stream API works, different formats for record
          additions and deletions so separate streams 
    - "Checkpoint file inventory record" is what establishes that checkpoint as the most recent valid one
        - Last thing that gets written into the SQL server log
        - LSN (log sequence number) logical clock doesn't match to txn time logical clock 
- Joe: "Piggybacking" of garbage collection onto worker threads allows you to still maintain a correct
  partial ordering while having it enforced by different actors
- Joe: Garbage collection in any relational system isn't at all like gc in the traditional programming
  language sense, there are no references so it's not about references, just about time visibility
- Ethan: Compiled procedures weren't worth it on disk-based systems since you were more I/O limited
- Yifan: Partitioning?
    - Joe: One way is having no runtime coordination between transactions by having static analysis at
      the query optimizer level only concurrently run txns that can't conflict, no latching/locking necessary
    - Other way is to do spatial locality where workers are only allowed to access certain parts of the table,
      again no latches/locks necessary, but this only works if your workload is perfectly partitioned 
- Ethan: Didn't treat related work very well? What was the state of the art before this?
    - Looking at what things here are new
        - Compiled queries: not a new idea
            - Having the traversal engine not knowing anything about schemas and just having
              schema-specific callbacks might be new
        - Transaction ids in timestamp then being replaced by logical time is probably new (except Postgres?)
        - Threading of indexes into the record format is unique
        - Slightly new ideas of how to use the log / checkpoints 
        - Main memory databases in general go back to 1990s though
- They seem to have thrown away the leaf level of the Bw-tree here, instead replacing it with the 
  linked list of records
- Joe: Very methodical about microbenchmarks, very practical approach that guided what they built 
- Johann: Is it really necessary to both make the code run really fast, and remove coordination?
  If we just made the code run really fast, they wouldn't hold locks for long, maybe the locking wouldn't
  be an issue? 
    - Discussion: This would still be an issue, no matter how much you sped it up there would always
      be some part that is slower, still would have coordination problems, locking overhead, etc.
- Ethan: How does their memory model work? When you make a write of a new txn, how quickly are
  the other cores able to see this?
- Non-progressive pieces of this system:
    - Have to update in-place the records when replacing txn ids with timestamps
    - Indexes have in-place updates but e.g. Bw-tree is *mostly* progressive
- Opportunity for more memory locality and such? Doing quite a bit of random jumping right now

## Bw-tree paper presented by Sanjay
- Joe: Would be interesting to be able to lay out this design space sufficiently such that
  a compiler could optimize these protocols, e.g. CAS is slow on this architecture so don't use
  that and do latching instead. Right now you have to basically completely redesign the tree each time. 
- Mutability centralized in the Mapping Table (PID -> physical address)
- Epochs for garbage collection scheme are another source of time
- B-link tree: B-tree where you only have to latch one page at a time. Bw-tree: no latching
- Bw-tree: possible to get lost, you may need to retraverse upwards and go back down again 
- Log looks similar to Postgres!
- Both of these papers still store all data (through an infinite transaction log), but don't provide
  any way to access it. Could you just build some sort of metadata/index on top of these to access?
- ops/sec drops off very quickly as the length of the delta chains grows (sweet spot is important)
- Joe: "Garbage collection" (in this case the delta chain compaction) handled continuously as-you-go
  instead of all-at-once (e.g. like in Postgres) is a very important aspect of both of these papers 
- Sanjay: Lots of workload dependency here

## Overview of next week
- Places where you locate things, e.g. what is mutable and what is immutable, is very important
  in all of these things
- Automatic compilation of moving mutable state into a central location? (e.g. the mapping table in Bw-tree)
- Synthesizing protocols?
