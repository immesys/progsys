Notes

- please refer to the slides by Johann
(http://www.slideshare.net/jssm1th/an-architecture-for-agile-machine-learning-in-realtime-applications) for reference. 
- Omitted some discussions specific to ML.

# Background
- Work done with a big team over three years.
- Started around 2005 as a social network that pivoted towards discovery, and the example here is dating.
- Main theme is agility for ML.
- Presented at KDD

# Problem Statement
Please refer to slides.

**Q**: (While on the slide about the messy deployment cycles) why the need to update model with fresh data?
**A**: New information is a lot more relevant than old ones, and user behavior apparently changes
frequently enough (migh have been other points).

# Solution
- Key Insight: Traditional architecture: no history, just the present state. Event history is the most natural
  way to process, but not used widely.
    * **Q**: Events cooridnation?
      * Time is just wall-clock time
    * **Q**: Is this a problem?
      * Not really because so long as the numbers stay the same, the ML algorithm is
        reproducable, also for relevant interations they happen on client side and the relative order is preserved
- Key Benefit:
  * simplifies production and experimentation code
  * consistent and reproducable between engineers

- The event history helps additioally with:
    * monitoring
    * reproducing work since there are less numbers of steps.
    * wont have issues with causality given the promised time.
    * take new data but keep feature definition the same, we might get new coefficients, and it should
      only get better, but as it turned out it got worse. The reason is that there is no data for the
      bad decisions.

**Q**: Why use CSV format?
**A**: Using CSV is fine since it’s vector processed

**Q**: How much of this is just good software engineering?
**A**: Using software engineering practices to help with system design.

**Q**: does this generalize? This is relevant to progressive systems.
**A**: You still need access to the most recent state. Log structured FS are already handling logs.
Another area: history of time series but no time associated.

**Q**: Why are not more people doing this?
**A**:
- memory issues, but that’s getting cheaper
- privacy
- cannot delete?

**Q**: Is this fundamental?
**A**: We cannot run linear time algo on these?

More ideas: ?

# Extentions and Questions
- Multi-resolution
- Connection with Log-structured File Systems
- Ways to build apps, from MVC to *CQRS* (Command Query Responsibility Segregation).

- **Q**: How does event history scale?
  - cold start is solved by snapshotting
  - maintaining index to the past — there are existing techniques but still a challenge
  - new feature to replay a lot of history (and care about speed for data scientists’ efficiency)

- **Q**: how far do you want to go back? Maybe it will be better (since people/things change overtime)
    * approximate queries
    * cannot sample since it’s based on engagements.
    * replaying the log helps you peak ahead so that roll forward (generating training examples to
    transform to outcomes) is fast

- **Q**: Why wasn’t it build on Spark?
    * Streaming is very slow on Spark? Especially if it’s not click stream (?).
    * Also fine grained state updates so there would be a lot more updates for rolling forward history.
    * (More discussions here but lost track of concurrency discussions...)
